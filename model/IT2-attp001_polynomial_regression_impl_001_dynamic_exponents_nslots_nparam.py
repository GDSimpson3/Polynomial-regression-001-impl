# -*- coding: utf-8 -*-
"""ATTP001-Polynomial-regression-impl-001-Dynamic-Exponents-NSLOTS-NPARAM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1meAsTvZwneUh94glkgI1baV6iHtOWfDC

# Dataset
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import math

# X = 4 * np.random.rand(100,1)
# Y = 4 + 7* X +  + 6*X**2 + 7*np.random.randn(100,1)

# X = 4 * np.random.rand(100,1)  # X sampled in [0,4]
# Y = 4 + 7*X + 6*X**2 + 7*np.random.randn(100,1)

X = 10 * np.random.rand(100,1)
Y = 17 *np.sin(X) +  2*np.random.randn(100,1)

X = (X - X.min()) / (X.max() - X.min()) # NORMALISATION

X = np.clip(X, 1e-6, None) # NORMALISATION

"""# Dynamic Regression Functions (from Array)"""

def CustomRegression(RegressionFunction: list[list[int]], Xparam) -> int:
  PredictedValueConstant = 0
  for Term in RegressionFunction:
    PredictedValueConstant = PredictedValueConstant + (Term[0] * Xparam ** Term[1])

  return PredictedValueConstant

def ToString(RegressionFunction: list[list[int]]) -> str:
  StringOutput = f''
  for Term in RegressionFunction:
    StringOutput = StringOutput + f'{Term[0]} X ^ {Term[1]} + '
  return StringOutput

"""# Visualise Data Plot func


"""

def PlotGraph(RegressionFunction: list[list[int]]):

  X_line = np.linspace(X.min(), X.max(), 100) # Creates 100 evenly spaced points

  # 4. Calculate the predicted Y-values (the regression line)

  Y_pred = 0

  for Term in RegressionFunction:
    Y_pred = Y_pred + (Term[0] *X_line**Term[1])


  # Y_pred = (C1 *X_line**E1) + (C2 *X_line**E2) + (C3 * X_line ** E3)


  # Add titles and labels

  plt.figure(figsize=(10, 6))
  plt.scatter(X, Y)
  plt.plot(X_line, Y_pred, color='red', linewidth=2, label=f'Regression Line: {ToString(RegressionFunction)}')

  plt.title('Years of Experience vs. Salary')
  plt.xlabel('YearsExperience')
  plt.ylabel('Salary')
  plt.grid(True)
  plt.show()

PlotGraph([[1,0],[0,1],[0,1]])

"""# Mean Squared Error Calculator

we're using the same data throughout, hence no need of DYNAMIC dynamic functions
"""

# InputType: list[list[int]] = [[C1,E1],[C2,E2],[C3,E3]....]

def MSE(RegressionFunction: list[list[int]]):
  YStream = Y
  XStream = X
  Summation = 0
  for N in range(len(YStream)):

    ObservedValue = YStream[N]
    PredictedValue = CustomRegression(RegressionFunction, XStream[N].item())

    Error = ObservedValue - PredictedValue
    ErrorSquared = Error ** 2

    Summation += ErrorSquared
    # print(f'Observed Value = {ObservedValue} --- Predicted Value = {PredictedValue} --- Error = {Error}')

  return (Summation / len(YStream)).item()

MSE([[1,2],[5,6],[2,3]])

"""# Partial Derivatives
This is to look at the rate of change of the BiVariate MSE with respects to only 1 Parameter
"""

def PartialDerivativeCN(CD, ED, RegressionFunction: list[list[int]]): # Its okay to pass in CD & ED Twice, it'll be easier later
  YStream = Y
  XStream = X
  Summation = 0
  for N in range(len(YStream)):

    ObservedValue = YStream[N]
    PredictedValue = CustomRegression(RegressionFunction,XStream[N].item())

    Error = PredictedValue - ObservedValue

    ErrorXPartialDer = Error * XStream[N].item() ** ED

    Summation = Summation + ErrorXPartialDer
    # print(f'Observed Value = {ObservedValue} --- Predicted Value = {PredictedValue} --- Error = {Error}')

  return (2 * Summation / len(YStream)).item()

import math
def PartialDerivativeEN(CD,ED, RegressionFunction: list[list[int]]):
  YStream = Y
  XStream = X
  Summation = 0
  for N in range(len(YStream)):

    ObservedValue = YStream[N]
    PredictedValue = CustomRegression(RegressionFunction,XStream[N].item())

    Error = PredictedValue - ObservedValue
    if not XStream[N].item() <= 0:

      ErrorXPartialDer = Error * (CD * (XStream[N].item()**ED) * (math.log(XStream[N].item())))
    else:
      continue


    Summation = Summation + ErrorXPartialDer
    # print(f'Observed Value = {ObservedValue} --- Predicted Value = {PredictedValue} --- Error = {Error}')

  return (2 * Summation / len(YStream)).item()

"""# Parameters"""

# PARAMETERS

AlphaC = 0.1
AlphaE = 2e-5 # Make steps of Exponent, almost nano

MaxIterations = 10000
MaxDegree = 20

min_exp = -5 # PREVENT EXPONENT EXPLOSION
max_exp = 20 # PREVENT EXPONENT EXPLOSION

WarmupPeriod = int(0.2 * MaxIterations)

lambda_L1 = 0.01 # L1 REGULARISATION PRUNING RATE (SMALL = LESS, LARGE = HEAVY)
PruneFrequency = 100 # Prune every 100
prune_threshold = 1e-5

ExponentDuplication_Tollerance = 1e-3 # Removing Duplicated Terms - How big should the difference be?
CoEfficientDuplication_Tollerance = 1e-2 # Removing Duplicated Terms - How big should the difference be for it to be gone?

LiveUpdateFrequency = 100

"""# Gradient Descent Iteration Steps"""

from IPython import display

RegressionTerms = []



for D in range(MaxDegree):
  RegressionTerms.append([1 * (10 ** (-D / 3)),D])




# PlotGraph(RegressionTerms)

MSE_History = [MSE(RegressionTerms)]

for I in range(MaxIterations):

  # PRE COMPUTING PARTIAL DERIVATIVES (Stability and avoding miditeration changes)

  PartialDerivatives = []

  for TermIndex in range(len(RegressionTerms)):
      tempParDerArr = []
      tempParDerArr.append(PartialDerivativeCN(RegressionTerms[TermIndex][0],RegressionTerms[TermIndex][1], RegressionTerms) - lambda_L1 * np.sign(RegressionTerms[TermIndex][0])) # L1 REGULARISATION
      tempParDerArr.append(PartialDerivativeEN(RegressionTerms[TermIndex][0],RegressionTerms[TermIndex][1], RegressionTerms))

      PartialDerivatives.append(tempParDerArr)


  for TermIndex in range(len(RegressionTerms)):
        RegressionTerms[TermIndex][0] = RegressionTerms[TermIndex][0] - (AlphaC * PartialDerivatives[TermIndex][0])
        RegressionTerms[TermIndex][1] = RegressionTerms[TermIndex][1] - (AlphaE * PartialDerivatives[TermIndex][1])


        RegressionTerms[TermIndex][1] = np.clip(RegressionTerms[TermIndex][1], min_exp, max_exp) # PREVENT EXPONENT EXPLOSION, USE CLIP, BASICALLY IF BIGGER THAN 8, IT BECOMES 8, IF ITS SMALLER THAN -5, IT BECOMES -5


  if I > WarmupPeriod and I % PruneFrequency == 0:
    RegressionTerms = [t for t in RegressionTerms if abs(t[0]) > prune_threshold] # PRUNE COEFFICIENTS THAT ARE CLOSE TO THRESHOLD (close to 0)



    # Removing Duplicated Terms - How big should the difference be?
    unique_terms = []
    for TermIndex in range(len(RegressionTerms)):
        duplicate = False
        for CompareCoefficient, CompareExponent in unique_terms:
          # abs(RegressionTerms[TermIndex][1] - CompareExponent) < ExponentDuplication_Tollerance
          # abs(RegressionTerms[TermIndex][0] - CompareCoefficient) < CoEfficientDuplication_Tollerance
            if abs(RegressionTerms[TermIndex][1] - CompareExponent) < ExponentDuplication_Tollerance and abs(RegressionTerms[TermIndex][0] - CompareCoefficient) < CoEfficientDuplication_Tollerance:
                duplicate = True
                break
        if not duplicate:
            unique_terms.append([RegressionTerms[TermIndex][0], RegressionTerms[TermIndex][1]])
    RegressionTerms = unique_terms


  # print(f'ITERATION: {I} --- MSE {MSE(RegressionTerms)} --- TERMS {ToString(RegressionTerms)}')

# 4. --- LIVE PLOTTING LOGIC ---
  if I % LiveUpdateFrequency == 0:
      display.clear_output(wait=True)  # Clears the previous plot

      # Create a subplot with 2 areas: the Regression Fit and the MSE History
      fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

      # Area 1: The Fit
      # (Assuming PlotGraph logic here)
      ax1.scatter(X, Y, color='blue', alpha=0.5, label='Actual Data')
      # Generate X range for the curve
      x_range = np.linspace(X.min(), X.max(), 100)
      y_curve = [CustomRegression(RegressionTerms, x_val) for x_val in x_range]
      ax1.plot(x_range, y_curve, color='red', label='Current Fit')
      ax1.set_title(f"Iteration {I} | Terms: {len(RegressionTerms)}")
      ax1.legend()

      # Area 2: MSE History (Log scale is better for seeing convergence)
      ax2.plot(MSE_History, color='green')
      ax2.set_yscale('log')
      ax2.set_title(f"Current MSE: {MSE(RegressionTerms):.4f}")
      ax2.set_xlabel("Iterations")

      plt.show() # Renders the current state

  MSE_History.append(MSE(RegressionTerms))





print(f'Final Parameters give an MSE Of {MSE(RegressionTerms)}, Parameters are {ToString(RegressionTerms)}')

# PlotGraph(RegressionTerms)

PlotGraph(RegressionTerms)

plt.plot(MSE_History)
plt.xlabel("Iteration")
plt.ylabel("MSE")
plt.title("MSE During Gradient Descent")
plt.grid(True)
plt.show()